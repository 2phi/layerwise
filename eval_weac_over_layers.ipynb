{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89b0130",
   "metadata": {},
   "source": [
    "# Eval WEAC\n",
    "\n",
    "Initialize models, run over a resolution of 5cm with a standardized weak layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702d9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07d9a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(\"/home/pillowbeast/Documents/weac\")\n",
    "\n",
    "from weac.analysis import CriteriaEvaluator, CoupledCriterionResult, SSERRResult\n",
    "from weac.core.system_model import SystemModel\n",
    "from weac.components import ModelInput, Segment, ScenarioConfig, WeakLayer, CriteriaConfig\n",
    "from weac.utils.snowpilot_parser import SnowPilotParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4092ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_files = 100\n",
    "\n",
    "# Process multiple files\n",
    "file_paths = []\n",
    "for directory in os.listdir(\"data/snowpits\"):\n",
    "    for file in os.listdir(f\"data/snowpits/{directory}\"):\n",
    "        if file.endswith(\".xml\"):\n",
    "            file_paths.append(f\"data/snowpits/{directory}/{file}\")\n",
    "\n",
    "paths: List[str] = []\n",
    "parsers: List[SnowPilotParser] = []\n",
    "\n",
    "for file_path in file_paths[:number_of_files]:\n",
    "    snowpilot_parser = SnowPilotParser(file_path)\n",
    "    paths.append(file_path)\n",
    "    parsers.append(snowpilot_parser)\n",
    "\n",
    "print(f\"\\nFound {len(paths)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup standard values\n",
    "wl_spacing = 50 # mm\n",
    "phi = 0.0\n",
    "standard_scenario_config = ScenarioConfig(system_type=\"skier\", phi=phi)\n",
    "standard_weak_layer = WeakLayer(rho=125, h=20, E=1.0, sigma_c=6.16, tau_c=5.09)\n",
    "standard_segments = [\n",
    "    Segment(length=10000, has_foundation=True, m=0.0),\n",
    "    Segment(\n",
    "        length=10000,\n",
    "        has_foundation=True,\n",
    "        m=0.0,\n",
    "    ),\n",
    "]\n",
    "standard_criteria_config = CriteriaConfig()\n",
    "standard_criteria_evaluator = CriteriaEvaluator(standard_criteria_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import weac\n",
    "\n",
    "criteria_data_path = \"data/criteria_data\"\n",
    "os.makedirs(criteria_data_path, exist_ok=True)\n",
    "\n",
    "rerun = False\n",
    "path = paths[0]\n",
    "parser = parsers[0]\n",
    "\n",
    "data_rows = []\n",
    "# Extract layers\n",
    "layers, density_method = parser.extract_layers()\n",
    "print(\"layers: \", layers)\n",
    "heights = np.cumsum([layer.h for layer in layers])\n",
    "# space evenly and append the last height\n",
    "wl_depths = np.arange(wl_spacing, heights[-1], wl_spacing).tolist()\n",
    "wl_depths.append(heights[-1])\n",
    "\n",
    "# Extract layers for plotting\n",
    "plot_layers = layers\n",
    "plot_weaklayer = standard_weak_layer\n",
    "\n",
    "if rerun:\n",
    "    layers_copy = copy.deepcopy(layers)\n",
    "    for i, wl_depth in tqdm(enumerate(wl_depths), total=len(wl_depths), desc=\"Processing weak layers\", leave=False):\n",
    "        # only keep layers above the spacing\n",
    "        mask = heights <= wl_depth\n",
    "        new_layers = [layer for layer, keep in zip(layers_copy, mask) if keep]\n",
    "        # Add truncated layer if needed\n",
    "        depth = np.sum([layer.h for layer in new_layers]) if new_layers else 0.0\n",
    "        if depth < wl_depth:\n",
    "            additional_layer = copy.deepcopy(layers_copy[len(new_layers) if new_layers else 0])\n",
    "            additional_layer.h = wl_depth - depth\n",
    "            new_layers.append(additional_layer)\n",
    "        \n",
    "        if i >= len(wl_depths) - 2:\n",
    "            print(\"new_layer heights: \", [layer.h for layer in new_layers])\n",
    "            print(\"wl_depth: \", wl_depth)\n",
    "            print(\"new_layers: \", new_layers)\n",
    "        \n",
    "        model_input = ModelInput(\n",
    "            weak_layer=standard_weak_layer,\n",
    "            layers=new_layers,\n",
    "            scenario_config=standard_scenario_config,\n",
    "            segments=standard_segments,\n",
    "        )\n",
    "        system = SystemModel(model_input=model_input)\n",
    "        \n",
    "        cc_result: CoupledCriterionResult = standard_criteria_evaluator.evaluate_coupled_criterion(system, print_call_stats=True)\n",
    "\n",
    "        # Setup the scenario with the touchdown distance\n",
    "        time1 = time.time()\n",
    "        sserr_result: SSERRResult = standard_criteria_evaluator.evaluate_SSERR(system, vertical=False)\n",
    "\n",
    "        data_rows.append({\n",
    "            \"wl_depth\": wl_depth,\n",
    "            \"impact_criterion\": cc_result.initial_critical_skier_weight,\n",
    "            \"coupled_criterion\": cc_result.critical_skier_weight,\n",
    "            \"sserr_result\": sserr_result.SSERR,\n",
    "            \"touchdown_distance\": sserr_result.touchdown_distance,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    df.to_csv(os.path.join(criteria_data_path, os.path.basename(file_path)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56461958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly_snow_profile import snow_profile\n",
    "import pandas as pd\n",
    "\n",
    "if not rerun:\n",
    "    dataframe = pd.read_csv(\"data/criteria_data/snowpits-24461-caaml.csv\")\n",
    "else:\n",
    "    dataframe = df\n",
    "\n",
    "snow_profile_fig = snow_profile(weaklayer=plot_weaklayer, layers=plot_layers)\n",
    "snow_profile_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4978f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly_snow_profile import criticality_plots\n",
    "\n",
    "crit_plots_fig = criticality_plots(plot_weaklayer, plot_layers, dataframe)\n",
    "# crit_plots_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31980f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly_snow_profile import criticality_heatmap\n",
    "\n",
    "crit_hm_fig = criticality_heatmap(plot_weaklayer, plot_layers, dataframe)\n",
    "crit_hm_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad32184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataframe[\"wl_depth\"], dataframe[\"impact_criterion\"], label=\"Impact Criterion\")\n",
    "plt.plot(dataframe[\"wl_depth\"], dataframe[\"coupled_criterion\"], label=\"Coupled Criterion\")\n",
    "# plot vertical lines at the end of each layer\n",
    "for i, height in enumerate(heights):\n",
    "    plt.axvline(x=height, color=\"black\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataframe[\"wl_depth\"], dataframe[\"sserr_result\"], label=\"SSERR\")\n",
    "# plt.ylim(0, 4000)\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dataframe[\"wl_depth\"], dataframe[\"touchdown_distance\"], label=\"Touchdown Distance\")\n",
    "plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "figures = [crit_plots_fig, snow_profile_fig, crit_hm_fig]\n",
    "\n",
    "images = []\n",
    "for fig in figures:\n",
    "    width = fig.layout.width*2\n",
    "    height = fig.layout.height*2\n",
    "    img_bytes = fig.to_image(format=\"png\", width=width, height=height, scale=2)\n",
    "    image = Image.open(BytesIO(img_bytes))\n",
    "    images.append(image)\n",
    "\n",
    "total_width = sum(im.width for im in images)\n",
    "max_height = max(im.height for im in images)\n",
    "combined = Image.new(\"RGB\", (total_width, max_height), color=(255, 255, 255))\n",
    "x_offset = 0\n",
    "for im in images:\n",
    "    combined.paste(im, (x_offset, 0))\n",
    "    x_offset += im.width\n",
    "\n",
    "combined.save(\"plots/combined.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_weac_over_layers(parser: SnowPilotParser, scenario_config: ScenarioConfig, segments: list[Segment], weaklayer: WeakLayer, wl_spacing=100):\n",
    "    data_rows = []\n",
    "    # Extract layers\n",
    "    layers, density_method = parser.extract_layers()\n",
    "    heights = np.cumsum([layer.h for layer in layers])\n",
    "    # space evenly and append the last height\n",
    "    wl_depths = np.arange(wl_spacing, heights[-1], wl_spacing).tolist()\n",
    "    wl_depths.append(heights[-1])\n",
    "    \n",
    "    layers_copy = copy.deepcopy(layers)\n",
    "    for i, wl_depth in tqdm(enumerate(wl_depths), total=len(wl_depths), desc=\"Processing weak layers\", leave=False):\n",
    "        # only keep layers above the spacing\n",
    "        mask = heights <= wl_depth\n",
    "        new_layers = [layer for layer, keep in zip(layers_copy, mask) if keep]\n",
    "        # Add truncated layer if needed\n",
    "        depth = np.sum([layer.h for layer in new_layers]) if new_layers else 0.0\n",
    "        if depth < wl_depth:\n",
    "            additional_layer = copy.deepcopy(layers_copy[len(new_layers) if new_layers else 0])\n",
    "            additional_layer.h = wl_depth - depth\n",
    "            new_layers.append(additional_layer)\n",
    "        \n",
    "        model_input = ModelInput(\n",
    "            weak_layer=weaklayer,\n",
    "            layers=new_layers,\n",
    "            scenario_config=scenario_config,\n",
    "            segments=segments,\n",
    "        )\n",
    "        system = SystemModel(model_input=model_input)\n",
    "        \n",
    "        cc_result: CoupledCriterionResult = standard_criteria_evaluator.evaluate_coupled_criterion(system, print_call_stats=False)\n",
    "        sserr_result: SSERRResult = standard_criteria_evaluator.evaluate_SSERR(system, vertical=False, print_call_stats=False)\n",
    "\n",
    "        data_rows.append({\n",
    "            \"wl_depth\": wl_depth,\n",
    "            \"impact_criterion\": cc_result.initial_critical_skier_weight,\n",
    "            \"coupled_criterion\": cc_result.critical_skier_weight,\n",
    "            \"sserr_result\": sserr_result.SSERR,\n",
    "            \"touchdown_distance\": sserr_result.touchdown_distance,\n",
    "        })\n",
    "    return data_rows, layers, weaklayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def combine_plots(file_path: str, name: str, figures: list[go.Figure]):\n",
    "\n",
    "    images = []\n",
    "    for fig in figures:\n",
    "        width = fig.layout.width*2\n",
    "        height = fig.layout.height*2\n",
    "        img_bytes = fig.to_image(format=\"png\", width=width, height=height, scale=2)\n",
    "        image = Image.open(BytesIO(img_bytes))\n",
    "        images.append(image)\n",
    "\n",
    "    total_width = sum(im.width for im in images)\n",
    "    max_height = max(im.height for im in images)\n",
    "    combined = Image.new(\"RGB\", (total_width, max_height), color=(255, 255, 255))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        combined.paste(im, (x_offset, 0))\n",
    "        x_offset += im.width\n",
    "\n",
    "    combined.save(f\"{file_path}/{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6303e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup standard values\n",
    "wl_spacing = 50 # mm\n",
    "phi = 0.0\n",
    "standard_scenario_config = ScenarioConfig(system_type=\"skier\", phi=phi)\n",
    "standard_weak_layer = WeakLayer(rho=125, h=20, E=1.0, sigma_c=5.16, tau_c=4.09)\n",
    "standard_segments = [\n",
    "    Segment(length=10000, has_foundation=True, m=0.0),\n",
    "    Segment(\n",
    "        length=10000,\n",
    "        has_foundation=True,\n",
    "        m=0.0,\n",
    "    ),\n",
    "]\n",
    "standard_criteria_config = CriteriaConfig()\n",
    "standard_criteria_evaluator = CriteriaEvaluator(standard_criteria_config)\n",
    "\n",
    "scenario_config = standard_scenario_config\n",
    "segments = standard_segments\n",
    "weaklayer = standard_weak_layer\n",
    "\n",
    "plots_path = \"plots\"\n",
    "criteria_data_path = \"data/criteria_data\"\n",
    "os.makedirs(criteria_data_path, exist_ok=True)\n",
    "\n",
    "error_paths = []\n",
    "for i, (file_path, parser) in tqdm(\n",
    "    enumerate(zip(paths, parsers)), total=len(paths), desc=\"Processing files\"\n",
    "):  \n",
    "    try:\n",
    "        data_rows, layers, weaklayer = eval_weac_over_layers(parser, scenario_config, segments, weaklayer, wl_spacing=wl_spacing)\n",
    "        dataframe = pd.DataFrame(data_rows)\n",
    "        snow_profile_fig = snow_profile(weaklayer=weaklayer, layers=layers)\n",
    "        crit_plots_fig = criticality_plots(weaklayer, layers, dataframe)\n",
    "        crit_hm_fig = criticality_heatmap(weaklayer, layers, dataframe)\n",
    "        combine_plots(plots_path, os.path.basename(file_path), [crit_plots_fig, snow_profile_fig, crit_hm_fig])\n",
    "        base, _ = os.path.splitext(os.path.basename(file_path))\n",
    "        dataframe.to_csv(os.path.join(criteria_data_path, base + \".csv\"), index=False)\n",
    "    except Exception as e:\n",
    "        error_paths.append(file_path)\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Amount of errors: {len(error_paths)}\")\n",
    "print(\"Processed files: \", len(paths) - len(error_paths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layerwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
